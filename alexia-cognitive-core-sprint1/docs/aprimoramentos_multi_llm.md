/**
 * @modified_by Manus AI
 * @date 1 de junho de 2025
 * @description DocumentaÃ§Ã£o completa dos aprimoramentos do sistema multi-LLM
 * Inclui instruÃ§Ãµes de uso, integraÃ§Ã£o e manutenÃ§Ã£o dos novos recursos
 */

# DocumentaÃ§Ã£o dos Aprimoramentos do Sistema Multi-LLM

## Ãndice

1. [VisÃ£o Geral](#visÃ£o-geral)
2. [Sistema de Feedback Ativo](#sistema-de-feedback-ativo)
3. [CortexCache (Cache SemÃ¢ntico)](#cortexcache-cache-semÃ¢ntico)
4. [Logs Inteligentes por Modelo](#logs-inteligentes-por-modelo)
5. [IntegraÃ§Ã£o com Supabase](#integraÃ§Ã£o-com-supabase)
6. [IntegraÃ§Ã£o com Lovable](#integraÃ§Ã£o-com-lovable)
7. [ManutenÃ§Ã£o e Monitoramento](#manutenÃ§Ã£o-e-monitoramento)
8. [Troubleshooting](#troubleshooting)

## VisÃ£o Geral

Este documento detalha os trÃªs principais aprimoramentos implementados no sistema multi-LLM do Alex iA:

1. **Sistema de Feedback Ativo**: Coleta feedback do usuÃ¡rio (ðŸ‘ðŸ‘Ž e escala 1-5) para melhorar o roteamento entre modelos.
2. **CortexCache**: Sistema de cache semÃ¢ntico que evita chamadas redundantes Ã s APIs de LLM.
3. **Logs Inteligentes**: Registro detalhado de mÃ©tricas por modelo, incluindo tempo, custo e fallback.

Estes aprimoramentos trabalham em conjunto para melhorar a experiÃªncia do usuÃ¡rio, reduzir custos e otimizar o desempenho do sistema.

## Sistema de Feedback Ativo

### Componentes Principais

- **FeedbackSystem.tsx**: Componente React que exibe botÃµes ðŸ‘ðŸ‘Ž e escala 1-5 opcional.
- **FeedbackBatchProcessor.ts**: ServiÃ§o que processa feedback em lote e atualiza preferÃªncias do orquestrador.
- **Tabelas Supabase**: `llm_feedback`, `llm_orchestrator_preferences`, `llm_feedback_batch_processing`.

### Como Usar

1. **Adicionar o componente de feedback apÃ³s respostas LLM**:

```tsx
import { FeedbackSystem } from '@/components/FeedbackSystem';
import { useLLMRouter } from '@/hooks/useLLMRouter';

const ChatMessage = ({ message }) => {
  const { recordResponseContext } = useLLMRouter();
  
  // Registrar contexto da resposta para feedback
  const responseContext = {
    question: message.question,
    answer: message.answer,
    modelName: message.modelName,
    provider: message.provider,
    usedFallback: message.usedFallback,
    responseTime: message.responseTime,
    tokensUsed: message.tokensUsed,
    timestamp: new Date(),
    sessionId: 'session-123',
    userId: 'user-456'
  };
  
  return (
    <div className="message">
      <div className="content">{message.answer}</div>
      <FeedbackSystem context={responseContext} />
    </div>
  );
};
```

2. **Configurar processamento em lote**:

```tsx
import { FeedbackBatchProcessor } from '@/services/FeedbackBatchProcessor';

// Executar periodicamente (ex: a cada 24h)
const processFeedback = async () => {
  const processor = new FeedbackBatchProcessor();
  await processor.startBatchProcessing();
  const preferencesUpdated = await processor.processFeedback();
  console.log(`${preferencesUpdated} preferÃªncias atualizadas`);
};
```

3. **Verificar preferÃªncias atualizadas**:

As preferÃªncias atualizadas serÃ£o automaticamente consideradas pelo hook `useLLMRouter` na prÃ³xima inicializaÃ§Ã£o.

### MÃ©tricas e Monitoramento

- **Taxa de feedback**: Porcentagem de respostas que recebem feedback.
- **DistribuiÃ§Ã£o de avaliaÃ§Ãµes**: ProporÃ§Ã£o de ðŸ‘ vs ðŸ‘Ž e distribuiÃ§Ã£o na escala 1-5.
- **Impacto no roteamento**: MudanÃ§as nas preferÃªncias do orquestrador ao longo do tempo.

## CortexCache (Cache SemÃ¢ntico)

### Componentes Principais

- **SemanticCache.ts**: ServiÃ§o que gerencia o cache semÃ¢ntico de respostas LLM.
- **Tabelas Supabase**: `llm_response_cache`, `llm_cache_metrics`.
- **ExtensÃ£o pgvector**: Utilizada para armazenamento e busca de embeddings.

### Como Usar

1. **Inicializar o cache**:

```tsx
import { SemanticCache } from '@/services/SemanticCache';

const cache = new SemanticCache({
  similarityThreshold: 0.85, // Limiar de similaridade (0-1)
  maxCacheAge: 7 * 24 * 60 * 60 * 1000, // 7 dias em ms
  userId: 'user-123'
});
```

2. **Verificar cache antes de chamar LLM**:

```tsx
const handleQuestion = async (question: string) => {
  // Determinar tipo de tarefa
  const taskType = inferTaskType(question);
  
  // Verificar cache
  const cachedResponse = await cache.getCachedResponse(question, taskType);
  
  if (cachedResponse) {
    // Usar resposta em cache
    return {
      answer: cachedResponse,
      fromCache: true
    };
  }
  
  // Se nÃ£o encontrou no cache, chamar LLM
  const { modelConfig } = llmRouter.routeByTask(taskType);
  const response = await callLLM(question, modelConfig);
  
  // Adicionar ao cache
  await cache.cacheResponse(
    question,
    response.answer,
    taskType,
    modelConfig.modelName,
    modelConfig.provider,
    response.tokensUsed
  );
  
  return {
    answer: response.answer,
    fromCache: false
  };
};
```

3. **ManutenÃ§Ã£o do cache**:

```tsx
// Limpar cache expirado (executar periodicamente)
const cleanupCache = async () => {
  const itemsRemoved = await cache.cleanupExpiredCache();
  console.log(`${itemsRemoved} itens de cache expirados removidos`);
};

// Obter estatÃ­sticas de uso do cache
const getCacheStats = async () => {
  const stats = await cache.getCacheStats();
  console.log(`Taxa de hit: ${stats.hitRate * 100}%`);
  console.log(`Tokens economizados em mÃ©dia: ${stats.averageTokensSaved}`);
};
```

### MÃ©tricas e Monitoramento

- **Taxa de hit**: Porcentagem de consultas atendidas pelo cache.
- **Tokens economizados**: Quantidade de tokens (e custo) economizados pelo cache.
- **DistribuiÃ§Ã£o de similaridade**: Histograma de scores de similaridade para hits de cache.

## Logs Inteligentes por Modelo

### Componentes Principais

- **LLMLogger.ts**: ServiÃ§o que registra e analisa mÃ©tricas detalhadas por modelo.
- **Tabelas Supabase**: `llm_call_logs`, `llm_model_metrics`.
- **FunÃ§Ã£o cron**: AgregaÃ§Ã£o diÃ¡ria de mÃ©tricas para anÃ¡lise de tendÃªncias.

### Como Usar

1. **Inicializar o logger**:

```tsx
import { LLMLogger } from '@/services/LLMLogger';

const logger = new LLMLogger({
  enableRealTimeLogging: true,
  batchSize: 10,
  userId: 'user-123',
  sessionId: 'session-456'
});
```

2. **Registrar chamadas LLM**:

```tsx
const callLLMWithLogging = async (question: string, modelConfig) => {
  // Iniciar log
  const callId = await logger.logStart(
    modelConfig.modelName,
    modelConfig.provider,
    taskType,
    question,
    estimatedInputTokens
  );
  
  try {
    // Chamar LLM
    const startTime = Date.now();
    const response = await callLLM(question, modelConfig);
    const responseTime = Date.now() - startTime;
    
    // Finalizar log com sucesso
    await logger.logEnd(
      callId,
      response.answer.length,
      response.tokensOutput,
      'success',
      {
        usedFallback: false,
        cacheHit: false
      }
    );
    
    return response;
  } catch (error) {
    // Finalizar log com erro
    await logger.logEnd(
      callId,
      0,
      0,
      'error',
      {
        errorMessage: error.message
      }
    );
    
    // Tentar fallback
    const fallbackResponse = await callFallbackLLM(question);
    
    // Registrar uso de fallback
    await logger.logEnd(
      callId,
      fallbackResponse.answer.length,
      fallbackResponse.tokensOutput,
      'success',
      {
        usedFallback: true,
        fallbackReason: 'primary_model_error',
        fallbackModel: fallbackResponse.modelName
      }
    );
    
    return fallbackResponse;
  }
};
```

3. **Analisar mÃ©tricas**:

```tsx
// Obter mÃ©tricas por modelo
const getModelMetrics = async () => {
  const startDate = new Date();
  startDate.setDate(startDate.getDate() - 30); // Ãšltimos 30 dias
  
  const metrics = await logger.getMetricsByModel(startDate);
  
  // Exibir mÃ©tricas
  metrics.forEach(metric => {
    console.log(`Modelo: ${metric.modelName} (${metric.provider})`);
    console.log(`Chamadas totais: ${metric.totalCalls}`);
    console.log(`Taxa de sucesso: ${metric.successRate * 100}%`);
    console.log(`Tempo mÃ©dio de resposta: ${metric.avgResponseTime}ms`);
    console.log(`Custo total: $${metric.totalCost.toFixed(2)}`);
    console.log(`Taxa de fallback: ${metric.fallbackRate * 100}%`);
    console.log(`Taxa de cache hit: ${metric.cacheHitRate * 100}%`);
    console.log('---');
  });
};

// Obter mÃ©tricas de fallback
const getFallbackInsights = async () => {
  const fallbackMetrics = await logger.getFallbackMetrics();
  
  console.log(`Total de fallbacks: ${fallbackMetrics.totalFallbacks}`);
  console.log('RazÃµes de fallback:');
  Object.entries(fallbackMetrics.fallbacksByReason).forEach(([reason, count]) => {
    console.log(`- ${reason}: ${count}`);
  });
};

// Obter mÃ©tricas de custo
const getCostInsights = async () => {
  const costMetrics = await logger.getCostMetrics('week');
  
  console.log(`Custo total: $${costMetrics.totalCost.toFixed(2)}`);
  console.log('Custo por modelo:');
  Object.entries(costMetrics.costByModel).forEach(([model, cost]) => {
    console.log(`- ${model}: $${cost.toFixed(2)}`);
  });
};
```

### MÃ©tricas e Monitoramento

- **Performance**: Tempo mÃ©dio de resposta, P95, P99 por modelo.
- **Confiabilidade**: Taxa de sucesso, taxa de fallback, razÃµes de fallback.
- **Custo**: Custo total, custo por modelo, custo por tipo de tarefa.
- **TendÃªncias**: EvoluÃ§Ã£o das mÃ©tricas ao longo do tempo.

## IntegraÃ§Ã£o com Supabase

### ConfiguraÃ§Ã£o Inicial

1. **Aplicar schemas SQL**:

Acesse o Editor SQL do Supabase e execute os seguintes scripts:

- `/supabase/migrations/20250601_llm_feedback_schema.sql`
- `/supabase/migrations/20250601_semantic_cache_schema.sql`
- `/supabase/migrations/20250601_llm_logs_schema.sql`

2. **Habilitar extensÃ£o pgvector**:

```sql
CREATE EXTENSION IF NOT EXISTS vector;
```

3. **Configurar variÃ¡veis de ambiente**:

```
VITE_SUPABASE_URL=https://seu-projeto.supabase.co
VITE_SUPABASE_ANON_KEY=sua-chave-anon
VITE_OPENAI_API_KEY=sua-chave-openai
```

### ManutenÃ§Ã£o do Banco de Dados

- **Limpeza periÃ³dica**: Configurar jobs para remover dados antigos e manter o tamanho do banco gerenciÃ¡vel.
- **Backup**: Configurar backups regulares das tabelas crÃ­ticas.
- **Monitoramento**: Verificar regularmente o crescimento das tabelas e performance das queries.

## IntegraÃ§Ã£o com Lovable

### SincronizaÃ§Ã£o com GitHub

1. **Commit dos arquivos**:

```bash
git add src/components/FeedbackSystem.tsx
git add src/services/FeedbackBatchProcessor.ts
git add src/services/SemanticCache.ts
git add src/services/LLMLogger.ts
git add src/hooks/useLLMRouter.tsx
git add supabase/migrations/*.sql
git commit -m "feat: add feedback system, semantic cache and intelligent logs"
git push origin main
```

2. **Sincronizar Lovable com GitHub**:

- Abra o Lovable 2.0
- Selecione "Importar Projeto"
- Conecte ao GitHub e selecione o repositÃ³rio `alexia-cognitive-core`
- Aguarde a sincronizaÃ§Ã£o completa

### Uso no Lovable

1. **Adicionar componente de feedback**:

- Navegue atÃ© o componente de chat ou resposta LLM
- Adicione o componente `FeedbackSystem` apÃ³s cada resposta

2. **Integrar cache semÃ¢ntico**:

- Modifique o fluxo de processamento de perguntas para verificar o cache antes de chamar LLM
- Adicione lÃ³gica para armazenar respostas no cache

3. **Implementar logging**:

- Adicione chamadas ao `LLMLogger` nos pontos de entrada e saÃ­da de chamadas LLM
- Crie um dashboard para visualizar mÃ©tricas

## ManutenÃ§Ã£o e Monitoramento

### Tarefas PeriÃ³dicas

1. **Processamento de feedback**:

Configurar um job para executar diariamente:

```typescript
// Exemplo com setInterval (em produÃ§Ã£o, usar um scheduler mais robusto)
setInterval(async () => {
  const processor = new FeedbackBatchProcessor();
  await processor.startBatchProcessing();
  await processor.processFeedback();
}, 24 * 60 * 60 * 1000); // 24 horas
```

2. **Limpeza de cache**:

```typescript
// Executar semanalmente
setInterval(async () => {
  const cache = new SemanticCache();
  await cache.cleanupExpiredCache();
}, 7 * 24 * 60 * 60 * 1000); // 7 dias
```

3. **AnÃ¡lise de mÃ©tricas**:

```typescript
// Gerar relatÃ³rio semanal
const generateWeeklyReport = async () => {
  const logger = new LLMLogger();
  
  // Definir perÃ­odo
  const endDate = new Date();
  const startDate = new Date();
  startDate.setDate(startDate.getDate() - 7);
  
  // Obter mÃ©tricas
  const modelMetrics = await logger.getMetricsByModel(startDate, endDate);
  const fallbackMetrics = await logger.getFallbackMetrics();
  const costMetrics = await logger.getCostMetrics('day', startDate, endDate);
  
  // Gerar relatÃ³rio
  const report = {
    period: {
      start: startDate.toISOString(),
      end: endDate.toISOString()
    },
    models: modelMetrics,
    fallback: fallbackMetrics,
    cost: costMetrics
  };
  
  // Salvar ou enviar relatÃ³rio
  await saveWeeklyReport(report);
};
```

### Alertas

Configurar alertas para situaÃ§Ãµes crÃ­ticas:

1. **Custo excessivo**:
   - Alerta quando o custo diÃ¡rio exceder um limite predefinido
   - Exemplo: > $10/dia

2. **Taxa de fallback alta**:
   - Alerta quando a taxa de fallback exceder um limite
   - Exemplo: > 10% das chamadas

3. **Tempo de resposta degradado**:
   - Alerta quando o P95 do tempo de resposta aumentar significativamente
   - Exemplo: > 50% acima da mÃ©dia histÃ³rica

## Troubleshooting

### Problemas Comuns

1. **Feedback nÃ£o estÃ¡ sendo registrado**:
   - Verificar conexÃ£o com Supabase
   - Verificar permissÃµes da tabela `llm_feedback`
   - Verificar erros no console do navegador

2. **Cache semÃ¢ntico nÃ£o estÃ¡ funcionando**:
   - Verificar se a extensÃ£o pgvector estÃ¡ habilitada
   - Verificar se os embeddings estÃ£o sendo gerados corretamente
   - Verificar o limiar de similaridade (pode estar muito alto)

3. **Logs nÃ£o estÃ£o sendo registrados**:
   - Verificar conexÃ£o com Supabase
   - Verificar se o flush automÃ¡tico estÃ¡ funcionando
   - Verificar erros no console

4. **Processamento em batch nÃ£o estÃ¡ atualizando preferÃªncias**:
   - Verificar se hÃ¡ feedback suficiente para anÃ¡lise estatÃ­stica
   - Verificar permissÃµes da tabela `llm_orchestrator_preferences`
   - Verificar logs de erro do processador

### DiagnÃ³stico

1. **Verificar conexÃ£o com Supabase**:

```typescript
const testSupabaseConnection = async () => {
  const supabase = createClient(
    import.meta.env.VITE_SUPABASE_URL as string,
    import.meta.env.VITE_SUPABASE_ANON_KEY as string
  );
  
  const { data, error } = await supabase.from('llm_feedback').select('count');
  
  if (error) {
    console.error('Erro de conexÃ£o com Supabase:', error);
    return false;
  }
  
  console.log('ConexÃ£o com Supabase OK');
  return true;
};
```

2. **Verificar geraÃ§Ã£o de embeddings**:

```typescript
const testEmbeddingGeneration = async () => {
  const cache = new SemanticCache();
  const testText = 'Texto de teste para geraÃ§Ã£o de embedding';
  
  try {
    const embedding = await cache['generateEmbedding'](testText);
    console.log(`Embedding gerado com sucesso. DimensÃ£o: ${embedding.length}`);
    return true;
  } catch (error) {
    console.error('Erro na geraÃ§Ã£o de embedding:', error);
    return false;
  }
};
```

3. **Verificar processamento de feedback**:

```typescript
const testFeedbackProcessing = async () => {
  const processor = new FeedbackBatchProcessor();
  
  try {
    await processor.startBatchProcessing();
    const updated = await processor.processFeedback();
    console.log(`Processamento de feedback OK. ${updated} preferÃªncias atualizadas.`);
    return true;
  } catch (error) {
    console.error('Erro no processamento de feedback:', error);
    return false;
  }
};
```

### RecuperaÃ§Ã£o

1. **Reiniciar serviÃ§os**:

```typescript
const resetServices = () => {
  // Limpar dados temporÃ¡rios
  localStorage.clear();
  
  // Reiniciar instÃ¢ncias
  const logger = new LLMLogger();
  const cache = new SemanticCache();
  const processor = new FeedbackBatchProcessor();
  
  console.log('ServiÃ§os reiniciados');
};
```

2. **Reconstruir Ã­ndices**:

```sql
-- Reconstruir Ã­ndice de embeddings
REINDEX INDEX idx_llm_response_cache_embedding;

-- Reconstruir outros Ã­ndices
REINDEX TABLE llm_feedback;
REINDEX TABLE llm_call_logs;
```

3. **Limpar dados corrompidos**:

```typescript
const cleanupCorruptedData = async () => {
  const supabase = createClient(
    import.meta.env.VITE_SUPABASE_URL as string,
    import.meta.env.VITE_SUPABASE_ANON_KEY as string
  );
  
  // Remover logs com erros
  await supabase
    .from('llm_call_logs')
    .delete()
    .eq('status', 'error');
  
  // Remover embeddings nulos
  await supabase
    .from('llm_response_cache')
    .delete()
    .is('embedding', null);
  
  console.log('Dados corrompidos removidos');
};
```
