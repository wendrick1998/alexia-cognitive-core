
import type { DocumentChunk, MemoryChunk } from './types.ts';

export function logRetrievedChunks(
  documentChunks: DocumentChunk[],
  memoryChunks: MemoryChunk[],
  userMessage: string
): void {
  console.log('\nüîç ===== IN√çCIO CHUNKS RECUPERADOS =====');
  
  const retrievedChunksData = {
    document_chunks: documentChunks.map((chunk: any) => ({
      content: chunk.content,
      document_name: chunk.document_name || 'Documento sem nome',
      similarity_score: chunk.similarity,
      chunk_type: 'document'
    })),
    memory_chunks: memoryChunks.map((memory: any) => ({
      content: memory.content,
      source: memory.source || 'Sistema',
      similarity_score: memory.similarity,
      chunk_type: 'memory'
    })),
    total_chunks: documentChunks.length + memoryChunks.length,
    query: userMessage
  };
  
  console.log(JSON.stringify(retrievedChunksData, null, 2));
  console.log('üîç ===== FIM CHUNKS RECUPERADOS =====\n');
}

export function logPromptData(
  contextText: string,
  userMessage: string,
  fullPrompt: string
): void {
  console.log('\nü§ñ ===== IN√çCIO PROMPT COMPLETO PARA LLM =====');
  
  const promptData = {
    system_message: 'Voc√™ √© Alex iA, um assistente IA prestativo. Responda √† pergunta do usu√°rio baseando-se estritamente no contexto fornecido. Se a informa√ß√£o n√£o estiver no contexto, diga que n√£o encontrou a informa√ß√£o nos documentos atuais. Seja claro, conciso e √∫til.',
    retrieved_context: contextText,
    user_question: userMessage,
    full_prompt: fullPrompt,
    prompt_length: fullPrompt.length,
    model_used: 'gpt-4o-mini'
  };
  
  console.log(JSON.stringify(promptData, null, 2));
  console.log('ü§ñ ===== FIM PROMPT COMPLETO PARA LLM =====\n');

  console.log('=== PROMPT COMPLETO PARA LLM ===');
  console.log('Tamanho total do prompt:', fullPrompt.length, 'caracteres');
}
